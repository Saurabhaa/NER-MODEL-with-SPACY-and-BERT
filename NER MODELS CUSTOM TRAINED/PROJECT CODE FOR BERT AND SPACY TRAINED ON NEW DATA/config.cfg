[paths]
train = "./train.spacy"
dev = "./test.spacy"
vectors = null

[system]
gpu_allocator = "pytorch"

[nlp]
lang = "en"
pipeline = ["tok2vec", "ner"]
batch_size = 1000

[components]

[components.ner]
factory = "ner"

[components.tok2vec]
factory = "tok2vec"

[components.ner.model]
@architectures = "spacy.TransitionBasedParser.v2"
state_type = "ner"
extra_state_tokens = false
hidden_width = 64
maxout_pieces = 2
use_upper = false
nO = null
tok2vec = {"@architectures": "spacy.HashEmbedCNN.v1",
           "width": 96,
           "depth": 4,
           "embed_size": 2000,
           "window_size": 1,
           "maxout_pieces": 3,
           "subword_features": true,
           "pretrained_vectors": null}

[training]
train_corpus = "corpora.train"
dev_corpus = "corpora.dev"
seed = 0
gpu_allocator = "pytorch"
max_epochs = 20
dropout = 0.1
accumulate_gradient = 1
patience = 1600
max_steps = 20000
eval_frequency = 200
frozen_components = []
annotating_components = []
before_to_disk = null
before_update = null
logger = {"@loggers": "spacy.ConsoleLogger.v1"}

[training.batcher]
@batchers = "spacy.batch_by_padded.v1"
size = 1000
buffer = 256
discard_oversize = true

[training.optimizer]
@optimizers = "Adam.v1"
learn_rate = 0.001
beta1 = 0.9
beta2 = 0.999
eps = 1e-8
grad_clip = 1.0

[pretraining]
path = null
vectors = null
corpus = null

[evaluation]
@evaluators = "spacy.ner"
thresholds = [0.5]
scorer = {"@scorers": "spacy.ner_scorer.v1"}
